{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3bb2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,matthews_corrcoef,f1_score, brier_score_loss,average_precision_score, confusion_matrix,mean_squared_error,r2_score,precision_score\n",
    "from sklearn.linear_model import LogisticRegression,ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import scipy.stats as sts\n",
    "import json\n",
    "from sklearn.utils import check_X_y, safe_mask\n",
    "import functools\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch_geometric\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "import sklearn.exceptions as ske\n",
    "warnings.simplefilter(\"ignore\", category=ske.ConvergenceWarning)\n",
    "from pykeen.datasets import PrimeKG\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd36464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad35f19",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c5d48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_clinical_feature = pd.read_excel(\"./dataSet.xlsx\",dtype='object')\n",
    "\n",
    "continue_features = ['Age','Height', 'Weight', 'BMI', 'Systolic_blood_pressure', 'Diastolic_blood_pressure',\n",
    "                               'Heart_rate', 'Abdominal_circumference', 'Pelvis_circumference', 'Oxygen_saturation',\n",
    "                               'Years_as_a_smoker', 'Leukocytes', 'Erythrocytes', 'Haemoglobin', 'Haematocrit', 'MCV',\n",
    "                               'MCH', 'MCHC', 'RDW', 'Platelets', 'MPV',\n",
    "                                'Basophils_#', 'Neutrophils_#', 'Lymphocytes_#', 'Monocytes_#',\n",
    "                               'Eosinophils_#', 'Blood_sugar', 'Uric_acid', 'GGT', 'Total_bilirubin',\n",
    "                               'hsTnI', 'Triglycerides', 'Total_cholesterol', 'HDL_cholesterol', 'LDL_cholesterol',\n",
    "                               'CRP', 'hs_CRP', 'HbA1c_IFCC']\n",
    "\n",
    "ordinal_features = [ 'Score_FSSQ', 'Score_PHQ_9_4', 'Score_PREDIMED']\n",
    "\n",
    "binary_features = ['Sex','Family_history_of_cerebrovascular_disease',\n",
    "                                  'Family_history_of_cardiovascular_disease', 'Hypertension', 'Diabetes_mellitus',\n",
    "                                  'Hypercholesterolemia','Peripheral_artery_disease', 'Anaemia','Chronic kidney disease',\n",
    "                                  'Thrombophilia','Statins', 'NOA', 'Beta_blockers', 'ACE_Inhibitors', 'Sartans',\n",
    "                                  'Diuretics', 'Nitrates', 'Calcium_antagonists_dihydropyridines',\n",
    "                                  'Calcium_antagonists_not_dihydropyridines', 'ASA', 'Clopidogrel', 'Warfarin',\n",
    "                                  'Oral_hypoglycemics', 'Insulin', 'Anxiolytic_Antidepressant', 'n_3_PUFA', 'Ezetimibe',\n",
    "                                  'Menopause_andropause', 'Effects_of_other_external_causes',\n",
    "                                  'Gastritis_and_duodenitis', 'Intervertebral_disc_disorders',\n",
    "                                  'Hereditary_hemolytic_anemias', 'Hypothyroidism',\n",
    "                                  'Osteoporosis_osteopenia_and_pathological_fracture', 'Cholelithiasis_and_cholecystitis',\n",
    "                                  'Allergic_rhinitis', 'Thyroiditis', 'Depression', 'Glaucoma', 'Diseases_of_esophagus',\n",
    "                                  'Nontoxic_nodular_goiter', 'Thyrotoxicosis_with_or_without_goiter', 'Asthma',\n",
    "                                  'Chronic_airway_obstruction', 'Viral_hepatitis',\n",
    "                                  'Upper_gastrointestinal_congenital_anomalies', 'Parkinsons_disease',\n",
    "                                  'Chronic_liver_disease_and_cirrhosis', 'Diverticulosis_and_diverticulitis',\n",
    "                                  'Psoriasis_and_related_disorders', 'Hyperplasia_of_prostate','Sleep_disorders',\n",
    "                                  'Disorders_of_protein_plasma_amino_acid_transport_and_metabolism','Migraine',\n",
    "                                  'Other_diseases_of_blood_and_blood_forming_organs',\n",
    "                                  'Nonspecific_findings_on_examination_of_blood', 'Other_disorders_of_metabolism']\n",
    "\n",
    "categorical_features = ['Level_of_education', 'Marital_status', 'Work', 'Physical_activity_frequency',\n",
    "                                      'Smoking_habits', 'Smoking_daily_quantity']\n",
    "\n",
    "labels = ['Stenosis_area_percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ede99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8e4efa",
   "metadata": {},
   "source": [
    "# Utils function for training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de4a81",
   "metadata": {},
   "source": [
    "### 1) discretize the Coronary Artery Stenosis to create the label classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fbad67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dataset(dataset,label):\n",
    "    \n",
    "    # add a cad group\n",
    "    cad_group = []\n",
    "    for index,row in dataset.iterrows():\n",
    "        if row['Stenosis_area_percentage'] == 0:\n",
    "            cad_group.append(0)\n",
    "        elif (row['Stenosis_area_percentage'] > 0) and (row['Stenosis_area_percentage'] < 25):\n",
    "            cad_group.append(1)\n",
    "        elif (row['Stenosis_area_percentage'] >= 25) and (row['Stenosis_area_percentage'] < 50):\n",
    "            cad_group.append(2)\n",
    "        elif (row['Stenosis_area_percentage'] >= 50) and (row['Stenosis_area_percentage'] < 70):\n",
    "            cad_group.append(3)\n",
    "        elif (row['Stenosis_area_percentage'] >= 70):\n",
    "            cad_group.append(4)\n",
    "\n",
    "    dataset['cad_group'] = cad_group\n",
    "\n",
    "    # discretize the label\n",
    "    if label == \"2vsall\":\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(0,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(1,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(2,1)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(3,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(4,0)\n",
    "    elif label == \"m2vsall\":\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(0,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(1,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(2,1)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(3,1)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(4,1)\n",
    "    elif label == \"3vsall\":\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(0,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(1,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(2,0)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(3,1)\n",
    "        dataset['cad_group'] = dataset['cad_group'].replace(4,0)\n",
    "\n",
    "    del dataset['Stenosis_area_percentage']\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f243d",
   "metadata": {},
   "source": [
    "### 2) variables encoding for making categorical as one-hot and standardized numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical_encode_categorical(X_train, X_val, binary_features, categorical_features,continue_features,label):\n",
    "    \n",
    "    if ((len(binary_features) == 0) and (len(categorical_features) == 0)):\n",
    "        \n",
    "        # numerical and ordinal scaled with standardscaler\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        # TRAIN\n",
    "        df_train_numerical = X_train.loc[:, X_train.columns.isin(continue_features)].copy(True)\n",
    "        X_train_numerical_scaled = standard_scaler.fit_transform(df_train_numerical)\n",
    "        df_train_numerical_scaled = pd.DataFrame(X_train_numerical_scaled,columns=df_train_numerical.columns)\n",
    "        df_train_numerical_scaled['Sample_ID'] = X_train['Sample_ID']\n",
    "        df_train_numerical_scaled[label] = X_train[label]\n",
    "        # VAL\n",
    "        df_val_numerical = X_val.loc[:, X_val.columns.isin(continue_features)].copy(True)\n",
    "        X_val_numerical_scaled = standard_scaler.transform(df_val_numerical)\n",
    "        df_val_numerical_scaled = pd.DataFrame(X_val_numerical_scaled,columns=df_val_numerical.columns)\n",
    "        df_val_numerical_scaled['Sample_ID'] = X_val['Sample_ID']\n",
    "        df_val_numerical_scaled[label] = X_val[label]\n",
    "        \n",
    "        \n",
    "        X_train = df_train_numerical_scaled.loc[:, ~df_train_numerical_scaled.columns.isin(['Sample_ID','cad_group'])]\n",
    "        y_train = df_train_numerical_scaled['cad_group']\n",
    "        X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "        X_val = df_val_numerical_scaled.loc[:, ~df_val_numerical_scaled.columns.isin(['Sample_ID','cad_group'])]\n",
    "        y_val = df_val_numerical_scaled['cad_group']\n",
    "        X_val.columns = X_val.columns.astype(str)\n",
    "        \n",
    "        return df_train_numerical_scaled, df_val_numerical_scaled, X_train, y_train, X_val, y_val\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # numerical and ordinal scaled with standardscaler\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        # TRAIN\n",
    "        df_train_numerical = X_train.loc[:, X_train.columns.isin(continue_features)].copy(True)\n",
    "        X_train_numerical_scaled = standard_scaler.fit_transform(df_train_numerical)\n",
    "        df_train_numerical_scaled = pd.DataFrame(X_train_numerical_scaled,columns=df_train_numerical.columns)\n",
    "        df_train_numerical_scaled['Sample_ID'] = X_train['Sample_ID']\n",
    "        df_train_numerical_scaled[label] = X_train[label]\n",
    "        # VAL\n",
    "        df_val_numerical = X_val.loc[:, X_val.columns.isin(continue_features)].copy(True)\n",
    "        X_val_numerical_scaled = standard_scaler.transform(df_val_numerical)\n",
    "        df_val_numerical_scaled = pd.DataFrame(X_val_numerical_scaled,columns=df_val_numerical.columns)\n",
    "        df_val_numerical_scaled['Sample_ID'] = X_val['Sample_ID']\n",
    "        df_val_numerical_scaled[label] = X_val[label]\n",
    "    \n",
    "        #### binary values as K-1 values\n",
    "        # TRAIN\n",
    "        df_train_binary_dummies = pd.get_dummies(X_train[binary_features].astype(str),drop_first=True)\n",
    "        # VAL\n",
    "        df_val_binary_dummies = pd.get_dummies(X_val[binary_features].astype(str),drop_first=True)\n",
    "\n",
    "        union_binary_features = list(set(df_train_binary_dummies.columns).union(set(df_val_binary_dummies.columns)))\n",
    "        df_train_binary_dummies = df_train_binary_dummies.reindex(columns=union_binary_features, fill_value=0)\n",
    "        df_val_binary_dummies = df_val_binary_dummies.reindex(columns=union_binary_features, fill_value=0)\n",
    "\n",
    "        df_train_binary_dummies['Sample_ID'] = X_train['Sample_ID']\n",
    "        df_train_binary_dummies[label] = X_train[label]\n",
    "        df_val_binary_dummies['Sample_ID'] = X_val['Sample_ID']\n",
    "        df_val_binary_dummies[label] = X_val[label]\n",
    "\n",
    "        #### categorical as K values\n",
    "        # TRAIN\n",
    "        df_train_categorical_dummies = pd.get_dummies(X_train[categorical_features].astype(str),drop_first=False)\n",
    "\n",
    "        # VAL\n",
    "        df_val_categorical_dummies = pd.get_dummies(X_val[categorical_features].astype(str),drop_first=False)\n",
    "\n",
    "        union_categorical_features = list(set(df_train_categorical_dummies.columns).union(set(df_val_categorical_dummies.columns)))\n",
    "        df_train_categorical_dummies = df_train_categorical_dummies.reindex(columns=union_categorical_features, fill_value=0)\n",
    "        df_val_categorical_dummies = df_val_categorical_dummies.reindex(columns=union_categorical_features, fill_value=0)\n",
    "\n",
    "        df_train_categorical_dummies['Sample_ID'] = X_train['Sample_ID']\n",
    "        df_train_categorical_dummies[label] = X_train[label]\n",
    "        df_val_categorical_dummies['Sample_ID'] = X_val['Sample_ID']\n",
    "        df_val_categorical_dummies[label] = X_val[label]\n",
    "    \n",
    "        new_training = (df_train_binary_dummies.merge(df_train_categorical_dummies,on=[\"Sample_ID\",label])).merge(df_train_numerical_scaled,on=[\"Sample_ID\",label])\n",
    "        new_val = (df_val_binary_dummies.merge(df_val_categorical_dummies,on=[\"Sample_ID\",label])).merge(df_val_numerical_scaled,on=[\"Sample_ID\",label])\n",
    "            \n",
    "        X_train = new_training.loc[:, ~new_training.columns.isin(['Sample_ID','cad_group'])]\n",
    "        y_train = new_training['cad_group']\n",
    "        X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "        X_val = new_val.loc[:, ~new_val.columns.isin(['Sample_ID','cad_group'])]\n",
    "        y_val = new_val['cad_group']\n",
    "        X_val.columns = X_val.columns.astype(str)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return new_training, new_val, X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61078351",
   "metadata": {},
   "source": [
    "### 3) compute metric for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_compute_metrics(model, X, y_true, dataframe_results, n_th_iteration,i_th_fold,j_th_fold,\n",
    "                               parameters_combination, type_validation, task):\n",
    "\n",
    "\n",
    "    # make prediction\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    y_pred_prob = model.predict_proba(X)\n",
    "    \n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    wf1 = f1_score(y_true, y_pred,average='weighted')\n",
    "    auc = roc_auc_score(y_true, y_pred_prob[:,1])\n",
    "    prec = precision_score(y_true, y_pred,zero_division=0.0)\n",
    "    \n",
    "    if type_validation == \"val\":\n",
    "\n",
    "        # append a row on the dataframe\n",
    "        new_row = pd.Series({\"n_repetition\":n_th_iteration,\n",
    "                                \"outer_fold\":i_th_fold,\n",
    "                                \"inner_fold\":j_th_fold,\n",
    "                                \"hyperparameters_combination\":str(parameters_combination),\n",
    "                                \"validation_mcc\":mcc,\n",
    "                                \"validation_wROCAUC\":auc,\n",
    "                                    \"validation_wf1\":wf1,\n",
    "                                    \"validation_prec_ppv\":prec})\n",
    "\n",
    "        dataframe_results =   pd.concat([dataframe_results, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "    elif type_validation == \"test\":\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        aupr = average_precision_score(y_true, y_pred_prob[:,1])\n",
    "        auc = roc_auc_score(y_true, y_pred_prob[:,1])\n",
    "        brier = brier_score_loss(y_true, y_pred_prob[:,1])\n",
    "\n",
    "        cf = confusion_matrix(y_true, y_pred)\n",
    "        cf_string = \"TP: \" + str(cf[1][1]) + \" FP: \" + str(cf[0][1]) + \"TN: \" + str(cf[0][0]) + \" FN: \" + str(cf[1][0])\n",
    "        metrics_sens = str(cf[1][1] / (cf[1][1]+cf[1][0]))\n",
    "        metrics_spec = str(cf[0][0] / (cf[0][0]+cf[0][1]))\n",
    "        metrics_prec = str(cf[1][1] / (cf[1][1]+cf[0][1]))\n",
    "        metrics_npv = str(cf[0][0] / (cf[0][0]+cf[1][0]))    \n",
    "\n",
    "        # append a row on the dataframe\n",
    "        new_row = pd.Series({\"n_repetition\":n_th_iteration,\n",
    "                                \"outer_fold\":i_th_fold,\n",
    "                                \"hyperparameters_combination\":str(parameters_combination),\n",
    "                                \"test_accuracy\":accuracy,\n",
    "                                \"test_wROCAUC\":auc,\n",
    "                                \"test_mcc\":mcc,\n",
    "                                \"test_wPRAUC\":aupr,\n",
    "                                \"test_wF1\":wf1,\n",
    "                                \"test_brier\": brier,\n",
    "                                \"test_confusion_matrix\":cf_string,\n",
    "                                \"test_sens_rec\":metrics_sens,\n",
    "                                \"test_spec\":metrics_spec,\n",
    "                                \"test_prec_ppv\":metrics_prec,\n",
    "                                \"test_npv\":metrics_npv})\n",
    "\n",
    "        dataframe_results =   pd.concat([dataframe_results, new_row.to_frame().T], ignore_index=True)\n",
    "            \n",
    "    return dataframe_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc651b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8899c8c6",
   "metadata": {},
   "source": [
    "### 4) transpose the gene dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_and_index_gene_df(gene_df):\n",
    "    gene_expr_data = gene_df.copy(True)\n",
    "    gene_expr_data = gene_expr_data.T\n",
    "    gene_expr_data['Sample_ID'] = gene_expr_data.index\n",
    "    gene_expr_data.reset_index(drop=True)\n",
    "    return gene_expr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c0212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f07d13c",
   "metadata": {},
   "source": [
    "### 5) create the patients average representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLINICAL FEATURES\n",
    "\n",
    "def associate_patient_annotations_to_clinical_embeddings(node_df,\n",
    "                                                patient_id, \n",
    "                                                clinical_features_annotation,\n",
    "                                                entities_embeddings):\n",
    "    \n",
    "    patient_tensor = torch.zeros(len(clinical_features_annotation[patient_id]),2*entities_embeddings.shape[1])\n",
    "    \n",
    "    count_annotation = 0\n",
    "    \n",
    "    # clinical features\n",
    "    for clinical_annotation in clinical_features_annotation[patient_id]:\n",
    "        \n",
    "        # single annotation\n",
    "        if len(clinical_annotation)==1:\n",
    "            \n",
    "            if clinical_annotation[0] == \"Family history of cerebrovascular disease\":\n",
    "                if \"Family history of heart disease\" in clinical_annotation:\n",
    "                    continue\n",
    "                else:\n",
    "                    clinical_annotation_to_search = \"Family history of heart disease\"\n",
    "            else:\n",
    "                clinical_annotation_to_search = clinical_annotation[0]\n",
    "            \n",
    "            # extract nodeID\n",
    "            nodeID_annotation = node_df[node_df['node_name'] == clinical_annotation_to_search]['Node ID'].iloc[0]\n",
    "            \n",
    "            # extract the embedding from the pretrained model\n",
    "            patient_tensor[count_annotation] = torch.concatenate([torch.real(entities_embeddings[nodeID_annotation]),torch.imag(entities_embeddings[nodeID_annotation])])\n",
    "            \n",
    "        # multiple annotation\n",
    "        else:\n",
    "            \n",
    "            intermediate_annotation_tensor = torch.zeros(len(clinical_annotation),2*entities_embeddings.shape[1])\n",
    "            \n",
    "            for i_th, clinical_annotation_th in enumerate(clinical_annotation):\n",
    "                nodeID_annotation = node_df[node_df['node_name'] == clinical_annotation_th]['Node ID'].iloc[0]\n",
    "                \n",
    "                # extract the embedding from the pretrained model\n",
    "                intermediate_annotation_tensor[i_th] = torch.concatenate([torch.real(entities_embeddings[nodeID_annotation]),torch.imag(entities_embeddings[nodeID_annotation])])\n",
    "                \n",
    "            # simply average the intermediate annotation tensor\n",
    "            patient_tensor[count_annotation] = torch.mean(intermediate_annotation_tensor,axis=0)\n",
    "            \n",
    "        count_annotation+=1\n",
    "            \n",
    "    return patient_tensor\n",
    "\n",
    "\n",
    "#for the genes, the returned tensor depends on the variables used\n",
    "def from_gene_list_to_matrix_embedding(gene_list, entities_embeddings, dict_gene_alias):\n",
    "    \n",
    "    gene_var_tensor = torch.zeros(len(gene_list),2*entities_embeddings.shape[1])\n",
    "    \n",
    "    for count_gene, gene in enumerate(gene_list):\n",
    "        \n",
    "        gene_to_search = dict_gene_alias_mapping_primeKG[gene]\n",
    "          \n",
    "        if len(node_df[node_df['node_name'] == gene_to_search]['Node ID']) == 0:\n",
    "            gene_var_tensor[count_gene] = torch.ones(1,2*entities_embeddings.shape[1])\n",
    "        else:\n",
    "            nodeID_annotation = node_df[node_df['node_name'] == gene_to_search]['Node ID'].iloc[0]\n",
    "            gene_var_tensor[count_gene] = torch.concatenate([torch.real(entities_embeddings[nodeID_annotation]),torch.imag(entities_embeddings[nodeID_annotation])])\n",
    "\n",
    "    return gene_var_tensor\n",
    "\n",
    "\n",
    "def make_patient_representation(dataset, annotation_embedding,  gene_expr_features,\n",
    "                             dict_gene_alias_mapping_primeKG,\n",
    "                             dict_Sample_ID_clinical_annotation_tensor,\n",
    "                             clinical_features_not_mapped):\n",
    "    \n",
    "    dataset_tensor = torch.zeros(dataset.shape[0],2*annotation_embedding.shape[1] + len(clinical_features_not_mapped))\n",
    "    gene_var_tensor = from_gene_list_to_matrix_embedding(gene_expr_features, annotation_embedding,dict_gene_alias_mapping_primeKG)\n",
    "        \n",
    "    for count_id, patient_id in enumerate(dataset['Sample_ID']):\n",
    "        patient_clinical_var_tensor = dict_Sample_ID_clinical_annotation_tensor[patient_id]\n",
    "                    \n",
    "        patient_gene_expression_values = torch.tensor(dataset[dataset['Sample_ID'] == patient_id][gene_expr_features].values).reshape(-1,1)\n",
    "        patient_gene_expression_tensor = torch.mul(patient_gene_expression_values,gene_var_tensor)\n",
    "            \n",
    "        clinical_values_not_mapped = torch.tensor(dataset[dataset['Sample_ID'] == patient_id][clinical_features_not_mapped].values).reshape(-1)\n",
    "            \n",
    "        patient_tensor = torch.mean(torch.concatenate([patient_clinical_var_tensor,patient_gene_expression_tensor],axis=0),axis=0)\n",
    "    \n",
    "        patient_tensor = torch.concatenate([clinical_values_not_mapped,patient_tensor],axis=0)\n",
    "            \n",
    "        dataset_tensor[count_id] = patient_tensor\n",
    "    \n",
    "    return dataset_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec18381",
   "metadata": {},
   "source": [
    "### 6) load and extract pykeen embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_return_embeddings(model_path, dataset):\n",
    "\n",
    "    trained_best_model = torch.load(model_path)\n",
    "\n",
    "    entity_representation_modules: List['pykeen.nn.Representation'] = trained_best_model.entity_representations\n",
    "    relation_representation_modules: List['pykeen.nn.Representation'] = trained_best_model.relation_representations\n",
    "\n",
    "    entity_embeddings: pykeen.nn.Embedding = entity_representation_modules[0]\n",
    "    relation_embeddings: pykeen.nn.Embedding = relation_representation_modules[0]\n",
    "\n",
    "    entity_embedding_tensor: torch.FloatTensor = entity_embeddings().detach().cpu()\n",
    "    relation_embedding_tensor: torch.FloatTensor = relation_embeddings().detach().cpu()\n",
    "\n",
    "    entity_to_id = dataset.entity_to_id\n",
    "    relation_to_id = dataset.relation_to_id\n",
    "\n",
    "    return trained_best_model, entity_embedding_tensor, relation_embedding_tensor, entity_to_id, relation_to_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fb6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e853fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4ee98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60eb6c87",
   "metadata": {},
   "source": [
    "# reading file for KG embeddings strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1287683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the PrimeKG node file\n",
    "nodes = pd.read_csv(\"./PrimeKG/nodes.csv\")\n",
    "\n",
    "# define the path with the results\n",
    "results_path = \"./\"\n",
    "\n",
    "# RotatE embeddings\n",
    "best_RotatE_path = results_path + '/RotatE/200/best_model/replicates/replicate-00000/trained_model.pkl'\n",
    "best_RotatE_model, best_RotatE_embeddings_entities, best_RotatE_embeddings_relations, entity_to_id, relation_to_id = read_model_return_embeddings(best_RotatE_path, PrimeKG())\n",
    "\n",
    "# prepare the node df embeddings\n",
    "node_df = pd.DataFrame.from_dict({'node_name': list(entity_to_id.keys()),'Node ID': list(entity_to_id.values())})\n",
    "node_df = node_df.merge(nodes[['node_name','node_type']],on='node_name',how=\"left\")\n",
    "node_more_type = list(node_df['node_name'].value_counts()[node_df['node_name'].value_counts().values>1].index)\n",
    "\n",
    "# read the annotation from INTESTRATCAD\n",
    "with open('./clinical_annotations.pkl', 'rb') as fp:\n",
    "        dict_patient_clinical_feature_annotation_NOTincluding_normal_PrimeKG = pickle.load(fp) \n",
    "        \n",
    "import pickle\n",
    "with open('./dict_gene_alias_mapping_primeKG.pkl', 'rb') as fp:\n",
    "        dict_gene_alias_mapping_primeKG = pickle.load(fp) \n",
    "\n",
    "## CLINICAL FEATURES\n",
    "dict_Sample_ID_clinical_annotation_tensor = {}\n",
    "\n",
    "for id_patient in tqdm(dataset['Sample_ID']):\n",
    "    dict_Sample_ID_clinical_annotation_tensor[id_patient] = associate_patient_annotations_to_clinical_embeddings(node_df, \n",
    "                                                                 id_patient, \n",
    "                                           dict_patient_clinical_feature_annotation_NOTincluding_normal_PrimeKG,\n",
    "                                           best_RotatE_embeddings_entities    )\n",
    "\n",
    "\n",
    "# define the clinical features not mapped to concatenate as values to the final tensor\n",
    "# note that the reported variables name is the one obtained after the one-hot encoding\n",
    "clinical_features_not_mapped = ['Sex_1','Age','Weight','Height','Smoking_daily_quantity_0','Smoking_daily_quantity_1',\n",
    "                                'Smoking_daily_quantity_2','Smoking_daily_quantity_3','Physical_activity_frequency_0',\n",
    "                                'Physical_activity_frequency_1','Physical_activity_frequency_2',\n",
    "                                'Physical_activity_frequency_3','Work_0','Work_1','Work_2','Marital_status_0',\n",
    "                                'Marital_status_1','Marital_status_2','Level_of_education_0','Level_of_education_1',\n",
    "                                'Level_of_education_2']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f82f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31d338ed",
   "metadata": {},
   "source": [
    "# function for classification with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a2516b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_classification(random_state, classifier, path_splits, n_repetition, path_output, label, nfold,\n",
    "                     source, KG_emb, annotation_embedding):\n",
    "    \n",
    "    print(classifier)\n",
    "    dataframe_results_val = pd.DataFrame(columns = [\"n_repetition\",\"outer_fold\",\"inner_fold\",\n",
    "                                                    \"hyperparameters_combination\",\"validation_mcc\",\n",
    "                                                    \"validation_wROCAUC\",\"validation_wf1\",\"validation_prec_ppv\"])\n",
    "    dataframe_results_test = pd.DataFrame(columns = [\"n_repetition\",\"outer_fold\",\"hyperparameters_combination\",\"test_accuracy\",\"test_wROCAUC\",\n",
    "                                                     \"test_mcc\",\"test_wPRAUC\",\"test_wF1\",\"test_brier\",\n",
    "                                    \"test_confusion_matrix\", \"test_sens_rec\", \"test_spec\",\"test_prec_ppv\", \"test_npv\"])\n",
    "    \n",
    "    # repetition loop\n",
    "    for n in range(n_repetition):\n",
    "        rn = random_state + n\n",
    "        \n",
    "        # outer cross validation loop \n",
    "        for i in tqdm(range(nfold)):\n",
    "\n",
    "            # inner cross validation loop \n",
    "            for j in range(nfold):\n",
    "                \n",
    "                \n",
    "                if source == \"clinical_variables\":\n",
    "                    training_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_training_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + \".xlsx\"),label)\n",
    "                    validation_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_val_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + \".xlsx\"),label)\n",
    "\n",
    "\n",
    "                    # scale and dummy encodings, define training dataset and outcome\n",
    "                    training_set_scaled, validation_set_scaled, X_train, y_train, X_val, y_val = scale_numerical_encode_categorical(training_set, validation_set, \n",
    "                                                                        binary_features, categorical_features,\n",
    "                                                                        continue_features + ordinal_features,\n",
    "                                                                        \"cad_group\")\n",
    "                    \n",
    "                elif source == \"gene_expression\":\n",
    "\n",
    "                    training_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_training_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + \".xlsx\"),label)\n",
    "                    validation_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_val_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + \".xlsx\"),label)\n",
    "\n",
    "                    training_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_train_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + '.txt' , sep=\"\\t\"))\n",
    "                    validation_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_val_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + '.txt' , sep=\"\\t\"))\n",
    "\n",
    "                    training_set = training_set_gen.merge(training_set[['Sample_ID','cad_group']].sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "                    validation_set = validation_set_gen.merge(validation_set[['Sample_ID','cad_group']].sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "\n",
    "                    gene_expr_features = list(set(training_set.columns).difference(set(['Sample_ID','cad_group'])))\n",
    "\n",
    "                    # scale and dummy encodings, define training dataset and outcome\n",
    "                    training_set_scaled, validation_set_scaled, X_train, y_train, X_val, y_val = scale_numerical_encode_categorical(training_set, validation_set,  \n",
    "                                                                            [], [], gene_expr_features, \"cad_group\")\n",
    "\n",
    "\n",
    "                elif source == \"clinical_gene_expression\":\n",
    "\n",
    "                    # clinical var\n",
    "                    training_set_cli = create_label_dataset(pd.read_excel(path_splits + \"dataset_training_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + \".xlsx\"),label)\n",
    "                    validation_set_cli = create_label_dataset(pd.read_excel(path_splits + \"dataset_val_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + \".xlsx\"),label)\n",
    "\n",
    "                    training_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_train_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + '.txt' , sep=\"\\t\"))\n",
    "                    validation_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_val_\" + str(rn) + \"_\" + str(i) + \"_\" + str(j) + '.txt' , sep=\"\\t\"))\n",
    "\n",
    "                    training_set = training_set_gen.merge(training_set_cli.sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "                    validation_set = validation_set_gen.merge(validation_set_cli.sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "\n",
    "\n",
    "                    with open(path_splits + 'gene_selected_DEG_' + str(rn) + \"_\" + str(i) + \"_\" + str(j) + '.json', 'r') as file:\n",
    "                        gene_expr_features = json.load(file)\n",
    "\n",
    "                    training_set_scaled, validation_set_scaled, X_train, y_train, X_val, y_val = scale_numerical_encode_categorical(training_set, validation_set, \n",
    "                                                                                    binary_features, categorical_features,\n",
    "                                                                                    continue_features + ordinal_features+gene_expr_features,\n",
    "                                                                                    \"cad_group\")\n",
    "                    \n",
    "                    \n",
    "                    if KG_emb:\n",
    "\n",
    "                        X_train = torch.Tensor.numpy(make_patient_representation(training_set_scaled, \n",
    "                                                                            annotation_embedding,  \n",
    "                                                                            gene_expr_features,\n",
    "                                                                            dict_gene_alias_mapping_primeKG,\n",
    "                                                                            dict_Sample_ID_clinical_annotation_tensor,\n",
    "                                                                            clinical_features_not_mapped))\n",
    "\n",
    "                        X_val = torch.Tensor.numpy(make_patient_representation(validation_set_scaled, \n",
    "                                                                            annotation_embedding,  \n",
    "                                                                            gene_expr_features,\n",
    "                                                                            dict_gene_alias_mapping_primeKG,\n",
    "                                                                            dict_Sample_ID_clinical_annotation_tensor,\n",
    "                                                                            clinical_features_not_mapped))\n",
    "\n",
    "\n",
    "                # compute class weight\n",
    "                class_weight = compute_class_weight('balanced',classes=np.unique(y_train), y = y_train)\n",
    "                dict_class_weight = {}\n",
    "                for c in np.unique(y_train):\n",
    "                    dict_class_weight[c] = class_weight[c]\n",
    "\n",
    "                # create classifier for each combination in a grid search\n",
    "                if classifier == \"logistic_regression\":\n",
    "                    parameters_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10 , 20, 50, 100]}\n",
    "                    grid_search = ParameterGrid(parameters_grid)\n",
    "\n",
    "                    for parameters_combination in grid_search:\n",
    "\n",
    "                        model = LogisticRegression(penalty=\"l1\", solver = \"liblinear\", random_state = rn,\n",
    "                                                    C = parameters_combination['C'],\n",
    "                                                    class_weight = dict_class_weight,\n",
    "                                                    max_iter = 1000)\n",
    "\n",
    "                        model.fit(X_train, y_train)\n",
    "                        dataframe_results_val = eval_model_compute_metrics(model, X_val, y_val, dataframe_results_val, \n",
    "                                                                               n,i,j,parameters_combination, \"val\",\"classification\")\n",
    "\n",
    "                elif classifier == \"elastic_net\":\n",
    "                    parameters_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 50, 100],\n",
    "                                    'l1_ratio': [0.25, 0.5, 0.75]}\n",
    "                    grid_search = ParameterGrid(parameters_grid)\n",
    "\n",
    "                    for parameters_combination in grid_search:\n",
    "\n",
    "                        model = LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", random_state = rn,\n",
    "                                                l1_ratio = parameters_combination[\"l1_ratio\"],\n",
    "                                                C = parameters_combination['C'],\n",
    "                                                class_weight = dict_class_weight,  max_iter = 1000,n_jobs=-1)\n",
    "\n",
    "                        model.fit(X_train, y_train)\n",
    "                        dataframe_results_val = eval_model_compute_metrics(model, X_val, y_val, dataframe_results_val, \n",
    "                                                                               n,i,j,parameters_combination, \"val\",\"classification\")\n",
    "\n",
    "\n",
    "                elif classifier == \"svm\":\n",
    "                    parameters_grid = {'kernel': ['linear','rbf','poly'],\n",
    "                                        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 50, 100]}\n",
    "\n",
    "                    grid_search = ParameterGrid(parameters_grid)\n",
    "\n",
    "                    for parameters_combination in grid_search:\n",
    "                        model = SVC(kernel=parameters_combination[\"kernel\"], probability =True,\n",
    "                                    C=parameters_combination['C'],\n",
    "                                    class_weight = dict_class_weight, random_state = rn,max_iter = 1000)\n",
    "\n",
    "                        model.fit(X_train, y_train)\n",
    "                        dataframe_results_val = eval_model_compute_metrics(model, X_val, y_val, dataframe_results_val, \n",
    "                                                                            n,i,j,parameters_combination, \"val\",\"classification\")\n",
    "\n",
    "                elif classifier == \"random_forest\":\n",
    "\n",
    "                    parameters_grid = {'n_estimators': [100],\n",
    "                                        'max_depth': [3, 5, 7, 10],\n",
    "                                        'min_samples_leaf': [1, 3, 5, 10],\n",
    "                                        'bootstrap':[True, False],\n",
    "                                        'max_features':[0.25,0.5,0.75],\n",
    "                                        'criterion':[\"gini\", \"entropy\"],\n",
    "                                            }\n",
    "                    grid_search = ParameterGrid(parameters_grid)\n",
    "\n",
    "                    for parameters_combination in grid_search:\n",
    "                        model = RandomForestClassifier(random_state=rn, n_jobs= -1, max_features = parameters_combination[\"max_features\"],\n",
    "                                                        n_estimators = parameters_combination['n_estimators'],\n",
    "                                                    max_depth = parameters_combination['max_depth'],\n",
    "                                                    min_samples_leaf = parameters_combination['min_samples_leaf'],\n",
    "                                                    bootstrap = parameters_combination['bootstrap'],\n",
    "                                                    criterion = parameters_combination['criterion'],\n",
    "                                                        class_weight = dict_class_weight)\n",
    "\n",
    "                        model.fit(X_train, y_train)\n",
    "                        dataframe_results_val = eval_model_compute_metrics(model, X_val, y_val, dataframe_results_val, \n",
    "                                                                            n,i,j,parameters_combination, \"val\",\"classification\")\n",
    "\n",
    "                elif classifier == \"xgboost\":\n",
    "                    sample_weight_list = []\n",
    "                    for sample in y_train:\n",
    "                        if sample==0:\n",
    "                            sample_weight_list.append(class_weight[0])\n",
    "                        elif sample==1:\n",
    "                            sample_weight_list.append(class_weight[1])\n",
    "\n",
    "                    parameters_grid = {\"learning_rate\": [1e-2, 1e-1, 0.3],\n",
    "                                        \"n_estimators\": [100],\n",
    "                                        \"gamma\": [0.2, 2, 5] ,\n",
    "                                        \"colsample_bytree\": [0.25,0.5,0.75],\n",
    "                                        \"max_depth\":[3, 5, 7],\n",
    "                                        \"min_child_weight\":[1, 3, 5],\n",
    "                                        \"reg_alpha\": [10, 1, 0.1, 0.01],\n",
    "                                        \"subsample\":[1]}\n",
    "                    grid_search = ParameterGrid(parameters_grid)\n",
    "\n",
    "                    for parameters_combination in tqdm(grid_search):\n",
    "\n",
    "                        model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=rn, booster=\"gbtree\", n_jobs = -1,\n",
    "                                                learning_rate = parameters_combination['learning_rate'],\n",
    "                                                gamma =parameters_combination['gamma'],\n",
    "                                                reg_alpha =parameters_combination['reg_alpha'],\n",
    "                                                max_depth = parameters_combination['max_depth'],\n",
    "                                                min_child_weight = parameters_combination['min_child_weight'],\n",
    "                                                colsample_bytree = parameters_combination['colsample_bytree'],\n",
    "                                                n_estimators = parameters_combination['n_estimators'], early_stopping_rounds = 30, eval_metric=['logloss'],\n",
    "                                                subsample= parameters_combination['subsample'])\n",
    "\n",
    "                        model.fit(X_train, y_train,eval_set=[(X_val, y_val)], sample_weight=sample_weight_list ,verbose=0 )\n",
    "                        dataframe_results_val = eval_model_compute_metrics(model, X_val, y_val, dataframe_results_val, \n",
    "                                                                            n,i,j,parameters_combination, \"val\",\"classification\")\n",
    "\n",
    "                      \n",
    "            # compute mean validation metrics on the specific inner fold and select best hyperparams\n",
    "            best_hyperparms_idxmax = dataframe_results_val[dataframe_results_val['inner_fold']==j].groupby('hyperparameters_combination')['validation_mcc'].mean().reset_index()['validation_mcc'].idxmax()\n",
    "            best_hyperparms = eval(dataframe_results_val.loc[best_hyperparms_idxmax,]['hyperparameters_combination'])\n",
    "            print(best_hyperparms)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if source == \"clinical_variables\":\n",
    "\n",
    "                # read train_val and test set from the outer fold\n",
    "                training_val_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_train_val_\" + str(rn) + \"_\" + str(i) + \".xlsx\"),label)\n",
    "                test_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_test_\" + str(rn) + \"_\" + str(i) + \".xlsx\"),label)\n",
    "\n",
    "                # scale and dummy encodings, define training dataset and outcome\n",
    "                training_val_set_scaled, test_set_scaled, X_train_val, y_train_val, X_test, y_test = scale_numerical_encode_categorical(training_val_set, test_set, \n",
    "                                                        binary_features, categorical_features,continue_features+ordinal_features,\n",
    "                                                                           \"cad_group\")\n",
    "                \n",
    "            elif source == \"gene_expression\":\n",
    "                \n",
    "                # read train_val and test set from the outer fold\n",
    "                training_val_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_train_val_\" + str(rn) + \"_\" + str(i) + \".xlsx\"),label)\n",
    "                test_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_test_\" + str(rn) + \"_\" + str(i) + \".xlsx\"),label)\n",
    "\n",
    "                training_val_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_train_val_\" + str(rn) + \"_\" + str(i) + '.txt' , sep=\"\\t\"))\n",
    "                test_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_test_\" + str(rn) + \"_\" + str(i) + '.txt' , sep=\"\\t\"))\n",
    "\n",
    "                training_val_set = training_val_set_gen.merge(training_val_set[['Sample_ID','cad_group']].sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "                test_set = test_set_gen.merge(test_set[['Sample_ID','cad_group']].sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "    \n",
    "                gene_expr_features = list(set(training_val_set.columns).difference(set(['Sample_ID','cad_group'])))\n",
    "    \n",
    "                # scale and dummy encodings, define training dataset and outcome\n",
    "                training_val_set_scaled, test_set_scaled, X_train_val, y_train_val, X_test, y_test = scale_numerical_encode_categorical(training_val_set, test_set,  [], [], \n",
    "                                                                   gene_expr_features, \"cad_group\")\n",
    "            \n",
    "            \n",
    "            elif source == \"clinical_gene_expression\":\n",
    "            \n",
    "                # clinical var\n",
    "                training_val_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_train_val_\" + str(rn) + \"_\" + str(i) + \".xlsx\"),label)\n",
    "                test_set = create_label_dataset(pd.read_excel(path_splits + \"dataset_test_\" + str(rn) + \"_\" + str(i) + \".xlsx\"),label)\n",
    "\n",
    "                training_val_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_train_val_\" + str(rn) + \"_\" + str(i) + '.txt' , sep=\"\\t\"))\n",
    "                test_set_gen = transpose_and_index_gene_df(pd.read_csv(path_splits + \"dataset_gen_test_\" + str(rn) + \"_\" + str(i) +'.txt' , sep=\"\\t\"))\n",
    "\n",
    "                training_val_set = training_val_set_gen.merge(training_val_set.sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "                test_set = test_set_gen.merge(test_set.sort_values(by=\"Sample_ID\"), on =\"Sample_ID\")\n",
    "\n",
    "                with open(path_splits + 'gene_selected_DEG_' + str(rn) + \"_\" + str(i) + '.json', 'r') as file:\n",
    "                    gene_expr_features = json.load(file)\n",
    "\n",
    "                training_val_set_scaled, test_set_scaled, X_train_val, y_train_val, X_test, y_test = scale_numerical_encode_categorical(training_val_set, test_set, \n",
    "                                                                        binary_features, categorical_features,\n",
    "                                                                        continue_features + ordinal_features+gene_expr_features,\n",
    "                                                                        \"cad_group\")\n",
    "\n",
    "                if KG_emb:\n",
    "\n",
    "                    X_train_val = torch.Tensor.numpy(make_patient_representation(training_val_set_scaled, \n",
    "                                                                          annotation_embedding,  \n",
    "                                                                          gene_expr_features,\n",
    "                                                                          dict_gene_alias_mapping_primeKG,\n",
    "                                                                          dict_Sample_ID_clinical_annotation_tensor,\n",
    "                                                                          clinical_features_not_mapped))\n",
    "\n",
    "                    X_test = torch.Tensor.numpy(make_patient_representation(test_set_scaled, \n",
    "                                                                          annotation_embedding,  \n",
    "                                                                          gene_expr_features,\n",
    "                                                                          dict_gene_alias_mapping_primeKG,\n",
    "                                                                          dict_Sample_ID_clinical_annotation_tensor,\n",
    "                                                                          clinical_features_not_mapped))\n",
    "\n",
    "            # compute class weight\n",
    "            class_weight = compute_class_weight('balanced',classes=np.unique(y_train_val), y = y_train_val)\n",
    "            dict_class_weight = {}\n",
    "            for c in np.unique(y_train_val):\n",
    "                dict_class_weight[c] = class_weight[c]\n",
    "                \n",
    "            # create the model with the best hyperparams\n",
    "            if classifier == \"logistic_regression\":\n",
    "                model = LogisticRegression(penalty=\"l1\", solver = \"liblinear\", random_state = rn,\n",
    "                                                        C = best_hyperparms['C'],\n",
    "                                                        class_weight = dict_class_weight,\n",
    "                                                        max_iter = 1000)\n",
    "\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "                with open(path_output + classifier + 'best_model_' + str(rn) + '_' + str(i) + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "\n",
    "                dataframe_results_test = eval_model_compute_metrics(model, X_test, y_test, dataframe_results_test, \n",
    "                                                                                n,i,j,best_hyperparms, \"test\",\"classification\")\n",
    "\n",
    "            elif classifier == \"elastic_net\":\n",
    "\n",
    "                model = LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", random_state = rn,\n",
    "                                                    l1_ratio = best_hyperparms[\"l1_ratio\"],\n",
    "                                                    C = best_hyperparms['C'],\n",
    "                                                    class_weight = dict_class_weight,  max_iter = 1000,n_jobs=-1)\n",
    "\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "                with open(path_output + classifier + 'best_model_' + str(rn) + '_' + str(i) + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                dataframe_results_test = eval_model_compute_metrics(model, X_test, y_test, dataframe_results_test, \n",
    "                                                                                n,i,j,best_hyperparms, \"test\",\"classification\")\n",
    "\n",
    "\n",
    "            elif classifier == \"svm\":\n",
    "\n",
    "                model = SVC(kernel=best_hyperparms[\"kernel\"], C = best_hyperparms['C'], probability =True,\n",
    "                                        class_weight = dict_class_weight, random_state = rn,max_iter = 1000)\n",
    "\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "                with open(path_output + classifier + 'best_model_' + str(rn) + '_' + str(i) + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                dataframe_results_test = eval_model_compute_metrics(model, X_test, y_test, dataframe_results_test, \n",
    "                                                                                n,i,j,best_hyperparms, \"test\",\"classification\")\n",
    "\n",
    "            elif classifier == \"random_forest\":\n",
    "\n",
    "\n",
    "                model = RandomForestClassifier(random_state=rn, n_jobs= -1, max_features = best_hyperparms[\"max_features\"],\n",
    "                                                    n_estimators = best_hyperparms['n_estimators'],\n",
    "                                                        max_depth = best_hyperparms['max_depth'],\n",
    "                                                        min_samples_leaf = best_hyperparms['min_samples_leaf'],\n",
    "                                                        bootstrap = best_hyperparms['bootstrap'],\n",
    "                                                        criterion = best_hyperparms['criterion'],\n",
    "                                                            class_weight = dict_class_weight)\n",
    "\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "                with open(path_output + classifier + 'best_model_' + str(rn) + '_' + str(i) + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                dataframe_results_test = eval_model_compute_metrics(model, X_test, y_test, dataframe_results_test, \n",
    "                                                                                n,i,j,best_hyperparms, \"test\",\"classification\")\n",
    "\n",
    "            elif classifier == \"xgboost\":\n",
    "                sample_weight_list = []\n",
    "                for sample in y_train_val:\n",
    "                    if sample==0:\n",
    "                        sample_weight_list.append(class_weight[0])\n",
    "                    elif sample==1:\n",
    "                        sample_weight_list.append(class_weight[1])\n",
    "\n",
    "\n",
    "                model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=rn, booster=\"gbtree\", n_jobs = -1,\n",
    "                                                    learning_rate = best_hyperparms['learning_rate'],\n",
    "                                                    gamma =best_hyperparms['gamma'],\n",
    "                                                    reg_alpha =best_hyperparms['reg_alpha'],\n",
    "                                                    max_depth = best_hyperparms['max_depth'],\n",
    "                                                    min_child_weight = best_hyperparms['min_child_weight'],\n",
    "                                                    colsample_bytree = best_hyperparms['colsample_bytree'],\n",
    "                                                    n_estimators = best_hyperparms['n_estimators'],\n",
    "                                                    subsample= best_hyperparms['subsample'])\n",
    "\n",
    "                model.fit(X_train_val, y_train_val, sample_weight=sample_weight_list ,verbose=0 )\n",
    "                with open(path_output + classifier + 'best_model_' + str(rn) + '_' + str(i) + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                dataframe_results_test = eval_model_compute_metrics(model, X_test, y_test, dataframe_results_test, \n",
    "                                                                                n,i,j,best_hyperparms, \"test\",\"classification\")\n",
    "\n",
    "       \n",
    "        dataframe_results_test.to_excel(path_output + classifier + \"_df_test_results.xlsx\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277ab19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbb89c16",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc3b7e",
   "metadata": {},
   "source": [
    "### <font color='red'> Clinical features </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2af95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in [\"logistic_regression\", \"elastic_net\",\"svm\", \"random_forest\",\"xgboost\"]:\n",
    "    ML_classification(127, cl, \"./all_data/m25/\", 1, \"./results_all_data/m25/clinical_variables/\", \"m2vsall\", 10, 'clinical_variables', False,None)\n",
    "    ML_classification(127, cl, \"./all_data/m50/\", 1, \"./results_all_data/m50/clinical_variables/\", \"m3vsall\", 10, 'clinical_variables', False,None)\n",
    "    ML_classification(127, cl, \"./all_data/m70/\", 1, \"./results_all_data/m70/clinical_variables/\", \"4a4b5vsall\", 10, 'clinical_variables', False,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e984c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0bb4a3",
   "metadata": {},
   "source": [
    "### <font color='green'> Gene Expression </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in [\"logistic_regression\", \"elastic_net\",\"svm\", \"random_forest\",\"xgboost\"]:\n",
    "    ML_classification(127, cl, \"./all_data/m25/\", 1, \"./results_all_data/m25/\", \"m2vsall\", 10, \"gene_expression\", False,None)\n",
    "    ML_classification(127, cl, \"./all_data/m50/\", 1, \"./results_all_data/m50/\", \"m3vsall\", 10, \"gene_expression\", False,None)\n",
    "    ML_classification(127, cl, \"./all_data/m70/\", 1, \"./results_all_data/m70/\", \"4a4b5vsall\", 10, \"gene_expression\", False,None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aae77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e2e2d43",
   "metadata": {},
   "source": [
    "### <font color='blue'> Clinical variables and gene expression </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db920980",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in [\"logistic_regression\", \"elastic_net\",\"svm\", \"random_forest\",\"xgboost\"]:\n",
    "    ML_classification(127, cl, \"./all_data/m25/\", 1, \"./results_all_data/m25/\", \"m2vsall\", 10, \"clinical_gene_expression\",False,None)\n",
    "    ML_classification(127, cl, \"./all_data/m50/\", 1, \"./results_all_data/m50/\", \"m3vsall\", 10, \"clinical_gene_expression\",False,None)\n",
    "    ML_classification(127, cl, \"./all_data/m70/\", 1, \"./results_all_data/m70/\", \"4a4b5vsall\", 10, \"clinical_gene_expression\",False,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0123b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0355e16a",
   "metadata": {},
   "source": [
    "### Clinical variables and gene expression with <font color='blue'> KG emb </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea203d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cl in [\"logistic_regression\", \"elastic_net\",\"svm\", \"random_forest\",\"xgboost\"]:\n",
    "\n",
    "    ML_classification(127, cl, \"./all_data/m25/\", 1, \"./results_all_data/m25/KGembPCA_clinical_gene_DGE/\", \"m2vsall\", 10, \n",
    "                    \"clinical_gene_expression\",\"DGE_already_performed\",True,best_RotatE_embeddings_entities)\n",
    "    ML_classification(127, cl, \"./all_data/m50/\", 1, \"./results_all_data/m50/KGembPCA_clinical_gene_DGE/\", \"m3vsall\", 10, \n",
    "                    \"clinical_gene_expression\",\"DGE_already_performed\",True,best_RotatE_embeddings_entities)\n",
    "    ML_classification(127, cl, \"./all_data/m70/\", 1, \"./results_all_data/m70/KGembPCA_clinical_gene_DGE/\", \"4a4b5vsall\", 10, \n",
    "                    \"clinical_gene_expression\",\"DGE_already_performed\",True,best_RotatE_embeddings_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9038ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac7396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9bbeb7b",
   "metadata": {},
   "source": [
    "# Read results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbdaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sources = ['clinical_variables', 'gene_expression_DGE', 'clinical_gene_DGE','KG_emb_clinical_gene_DGE']\n",
    "\n",
    "test_metrics = ['test_accuracy', 'test_wROCAUC', 'test_mcc', 'test_wPRAUC', 'test_wF1', 'test_brier',\n",
    "                'test_sens_rec', 'test_spec','test_prec_ppv', 'test_npv']\n",
    "\n",
    "df_results_test_plot = pd.DataFrame(columns = [\"label\",\"source\",\"model\"] + test_metrics )\n",
    "df_mean_std_test = pd.DataFrame(columns = [\"label\",\"source\",\"model\"] + test_metrics )\n",
    "\n",
    "for source_ in sources:\n",
    "\n",
    "    for label in [\"m25\",\"m50\",\"m70\"]:\n",
    "\n",
    "        for model_name in [\"logistic_regression\",\"elastic_net\",\"svm\",\"random_forest\",\"xgboost\"]:\n",
    "\n",
    "            path_test_results = \"./results_all_data/\" + label + \"/\" + source_ + \"/\" + model_name + \"_df_test_results.xlsx\"\n",
    "\n",
    "            classifier_test_results = pd.read_excel(path_test_results)\n",
    "            classifier_test_results['model'] = [model_name for x in range(len(classifier_test_results))]\n",
    "            classifier_test_results['label'] = [label for x in range(len(classifier_test_results))]\n",
    "            classifier_test_results['source'] = [source_ for x in range(len(classifier_test_results))]\n",
    "\n",
    "            mean_df_model = np.mean(classifier_test_results[test_metrics],axis=0)\n",
    "            std_df_model = np.std(classifier_test_results[test_metrics],axis=0)\n",
    "\n",
    "            new_row = pd.Series({\"label\":label,\n",
    "                                 \"model\":model_name,\n",
    "                                 \"test_accuracy\":np.round(mean_df_model['test_accuracy'],3).astype(str) + \"\" + np.round(std_df_model['test_accuracy'],3).astype(str) ,\n",
    "                                 \"test_wROCAUC\":np.round(mean_df_model['test_wROCAUC'],3).astype(str) + \"\" + np.round(std_df_model['test_wROCAUC'],3).astype(str) ,\n",
    "                                 \"test_mcc\":np.round(mean_df_model['test_mcc'],3).astype(str) + \"\" + np.round(std_df_model['test_mcc'],3).astype(str) ,\n",
    "                                 \"test_wPRAUC\":np.round(mean_df_model['test_wPRAUC'],3).astype(str) + \"\" + np.round(std_df_model['test_wPRAUC'],3).astype(str) ,\n",
    "                                 \"test_wF1\":np.round(mean_df_model['test_wF1'],3).astype(str) + \"\" + np.round(std_df_model['test_wF1'],3).astype(str) ,\n",
    "                                 \"test_brier\": np.round(mean_df_model['test_brier'],3).astype(str) + \"\" + np.round(std_df_model['test_brier'],3).astype(str) ,\n",
    "                                 \"test_sens_rec\":np.round(mean_df_model['test_sens_rec'],3).astype(str) + \"\" + np.round(std_df_model['test_sens_rec'],3).astype(str) ,\n",
    "                                 \"test_spec\":np.round(mean_df_model['test_spec'],3).astype(str) + \"\" + np.round(std_df_model['test_spec'],3).astype(str) ,\n",
    "                                 \"test_prec_ppv\":np.round(mean_df_model['test_prec_ppv'],3).astype(str) + \"\" + np.round(std_df_model['test_prec_ppv'],3).astype(str) ,\n",
    "                                 \"test_npv\":np.round(mean_df_model['test_npv'],3).astype(str) + \"\" + np.round(std_df_model['test_npv'],3).astype(str) ,\n",
    "                                 \"source\":source_})\n",
    "            df_mean_std_test =   pd.concat([df_mean_std_test, new_row.to_frame().T], ignore_index=True)\n",
    "            df_results_test_plot = pd.concat([df_results_test_plot,classifier_test_results[[\"label\",\"source\",\"model\"] + test_metrics]])\n",
    "\n",
    "df_mean_std_test.to_excel(\"./CAS_prediction_results.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
